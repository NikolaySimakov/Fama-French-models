{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hS6HGnEGzzck"
      },
      "source": [
        "#### How to use Boosting\n",
        "\n",
        "`https://keeryang.github.io/papers/Profits_2021Aug28.pdf`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "8yp6rLzPzzcl"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "from lightgbm import plot_importance, LGBMRegressor\n",
        "import getFamaFrenchFactors as gff\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import mean_squared_error, r2_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "hMZTT49czzcm",
        "outputId": "d53edc38-b850-488e-ce13-2611e672f584"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Symbol</th>\n",
              "      <th>A</th>\n",
              "      <th>AAL</th>\n",
              "      <th>AAPL</th>\n",
              "      <th>ABBV</th>\n",
              "      <th>ABNB</th>\n",
              "      <th>ABT</th>\n",
              "      <th>ACGL</th>\n",
              "      <th>ACN</th>\n",
              "      <th>ADBE</th>\n",
              "      <th>ADI</th>\n",
              "      <th>...</th>\n",
              "      <th>WTW</th>\n",
              "      <th>WY</th>\n",
              "      <th>WYNN</th>\n",
              "      <th>XEL</th>\n",
              "      <th>XOM</th>\n",
              "      <th>XYL</th>\n",
              "      <th>YUM</th>\n",
              "      <th>ZBH</th>\n",
              "      <th>ZBRA</th>\n",
              "      <th>ZTS</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2010-01-04</th>\n",
              "      <td>20.122227</td>\n",
              "      <td>4.496877</td>\n",
              "      <td>6.470741</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>18.952162</td>\n",
              "      <td>7.994444</td>\n",
              "      <td>32.212460</td>\n",
              "      <td>37.090000</td>\n",
              "      <td>23.694084</td>\n",
              "      <td>...</td>\n",
              "      <td>52.883579</td>\n",
              "      <td>9.905468</td>\n",
              "      <td>41.963718</td>\n",
              "      <td>12.918809</td>\n",
              "      <td>43.185623</td>\n",
              "      <td>NaN</td>\n",
              "      <td>19.158102</td>\n",
              "      <td>52.587051</td>\n",
              "      <td>28.670000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-01-05</th>\n",
              "      <td>19.903643</td>\n",
              "      <td>5.005957</td>\n",
              "      <td>6.481929</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>18.799042</td>\n",
              "      <td>7.967778</td>\n",
              "      <td>32.411549</td>\n",
              "      <td>37.700001</td>\n",
              "      <td>23.656675</td>\n",
              "      <td>...</td>\n",
              "      <td>52.765053</td>\n",
              "      <td>10.115747</td>\n",
              "      <td>44.515926</td>\n",
              "      <td>12.765595</td>\n",
              "      <td>43.354244</td>\n",
              "      <td>NaN</td>\n",
              "      <td>19.092571</td>\n",
              "      <td>54.251759</td>\n",
              "      <td>28.620001</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-01-06</th>\n",
              "      <td>19.832930</td>\n",
              "      <td>4.798554</td>\n",
              "      <td>6.378825</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>18.903446</td>\n",
              "      <td>7.933333</td>\n",
              "      <td>32.756096</td>\n",
              "      <td>37.619999</td>\n",
              "      <td>23.611784</td>\n",
              "      <td>...</td>\n",
              "      <td>53.614498</td>\n",
              "      <td>10.003899</td>\n",
              "      <td>43.932011</td>\n",
              "      <td>12.790110</td>\n",
              "      <td>43.728970</td>\n",
              "      <td>NaN</td>\n",
              "      <td>18.956089</td>\n",
              "      <td>54.234219</td>\n",
              "      <td>28.400000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-01-07</th>\n",
              "      <td>19.807215</td>\n",
              "      <td>4.939964</td>\n",
              "      <td>6.367033</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>19.060045</td>\n",
              "      <td>7.886667</td>\n",
              "      <td>32.725471</td>\n",
              "      <td>36.889999</td>\n",
              "      <td>23.424749</td>\n",
              "      <td>...</td>\n",
              "      <td>53.456463</td>\n",
              "      <td>9.959157</td>\n",
              "      <td>44.870213</td>\n",
              "      <td>12.734954</td>\n",
              "      <td>43.591564</td>\n",
              "      <td>NaN</td>\n",
              "      <td>18.950626</td>\n",
              "      <td>55.478374</td>\n",
              "      <td>27.690001</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2010-01-08</th>\n",
              "      <td>19.800785</td>\n",
              "      <td>4.845691</td>\n",
              "      <td>6.409364</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>19.157482</td>\n",
              "      <td>7.871111</td>\n",
              "      <td>32.595306</td>\n",
              "      <td>36.689999</td>\n",
              "      <td>23.559416</td>\n",
              "      <td>...</td>\n",
              "      <td>53.397202</td>\n",
              "      <td>9.867439</td>\n",
              "      <td>44.548744</td>\n",
              "      <td>12.741086</td>\n",
              "      <td>43.416687</td>\n",
              "      <td>NaN</td>\n",
              "      <td>18.956089</td>\n",
              "      <td>54.313072</td>\n",
              "      <td>27.600000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 503 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "Symbol              A       AAL      AAPL  ABBV  ABNB        ABT      ACGL  \\\n",
              "Date                                                                         \n",
              "2010-01-04  20.122227  4.496877  6.470741   NaN   NaN  18.952162  7.994444   \n",
              "2010-01-05  19.903643  5.005957  6.481929   NaN   NaN  18.799042  7.967778   \n",
              "2010-01-06  19.832930  4.798554  6.378825   NaN   NaN  18.903446  7.933333   \n",
              "2010-01-07  19.807215  4.939964  6.367033   NaN   NaN  19.060045  7.886667   \n",
              "2010-01-08  19.800785  4.845691  6.409364   NaN   NaN  19.157482  7.871111   \n",
              "\n",
              "Symbol            ACN       ADBE        ADI  ...        WTW         WY  \\\n",
              "Date                                         ...                         \n",
              "2010-01-04  32.212460  37.090000  23.694084  ...  52.883579   9.905468   \n",
              "2010-01-05  32.411549  37.700001  23.656675  ...  52.765053  10.115747   \n",
              "2010-01-06  32.756096  37.619999  23.611784  ...  53.614498  10.003899   \n",
              "2010-01-07  32.725471  36.889999  23.424749  ...  53.456463   9.959157   \n",
              "2010-01-08  32.595306  36.689999  23.559416  ...  53.397202   9.867439   \n",
              "\n",
              "Symbol           WYNN        XEL        XOM  XYL        YUM        ZBH  \\\n",
              "Date                                                                     \n",
              "2010-01-04  41.963718  12.918809  43.185623  NaN  19.158102  52.587051   \n",
              "2010-01-05  44.515926  12.765595  43.354244  NaN  19.092571  54.251759   \n",
              "2010-01-06  43.932011  12.790110  43.728970  NaN  18.956089  54.234219   \n",
              "2010-01-07  44.870213  12.734954  43.591564  NaN  18.950626  55.478374   \n",
              "2010-01-08  44.548744  12.741086  43.416687  NaN  18.956089  54.313072   \n",
              "\n",
              "Symbol           ZBRA  ZTS  \n",
              "Date                        \n",
              "2010-01-04  28.670000  NaN  \n",
              "2010-01-05  28.620001  NaN  \n",
              "2010-01-06  28.400000  NaN  \n",
              "2010-01-07  27.690001  NaN  \n",
              "2010-01-08  27.600000  NaN  \n",
              "\n",
              "[5 rows x 503 columns]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stocks_df = pd.read_csv('../../data/market_data/sp500/sp500_stocks.csv')\n",
        "df = stocks_df.pivot(\n",
        "    index='Date', columns='Symbol', values='Adj Close')\n",
        "\n",
        "df = df.reset_index()\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df = df.set_index('Date')\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "fOdukT1Ezzcm"
      },
      "outputs": [],
      "source": [
        "tickers = ['AAPL']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "CbKClwyCzzcm",
        "outputId": "127ed4b7-27b4-4971-b069-db05031e5d40"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date_ff_factors</th>\n",
              "      <th>Mkt-RF</th>\n",
              "      <th>SMB</th>\n",
              "      <th>HML</th>\n",
              "      <th>RMW</th>\n",
              "      <th>CMA</th>\n",
              "      <th>RF</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1963-07-31</td>\n",
              "      <td>-0.0039</td>\n",
              "      <td>-0.0041</td>\n",
              "      <td>-0.0097</td>\n",
              "      <td>0.0068</td>\n",
              "      <td>-0.0118</td>\n",
              "      <td>0.0027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1963-08-31</td>\n",
              "      <td>0.0507</td>\n",
              "      <td>-0.0080</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>0.0036</td>\n",
              "      <td>-0.0035</td>\n",
              "      <td>0.0025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1963-09-30</td>\n",
              "      <td>-0.0157</td>\n",
              "      <td>-0.0052</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>-0.0071</td>\n",
              "      <td>0.0029</td>\n",
              "      <td>0.0027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1963-10-31</td>\n",
              "      <td>0.0253</td>\n",
              "      <td>-0.0139</td>\n",
              "      <td>-0.0010</td>\n",
              "      <td>0.0280</td>\n",
              "      <td>-0.0201</td>\n",
              "      <td>0.0029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1963-11-30</td>\n",
              "      <td>-0.0085</td>\n",
              "      <td>-0.0088</td>\n",
              "      <td>0.0175</td>\n",
              "      <td>-0.0051</td>\n",
              "      <td>0.0224</td>\n",
              "      <td>0.0027</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  date_ff_factors  Mkt-RF     SMB     HML     RMW     CMA      RF\n",
              "0      1963-07-31 -0.0039 -0.0041 -0.0097  0.0068 -0.0118  0.0027\n",
              "1      1963-08-31  0.0507 -0.0080  0.0180  0.0036 -0.0035  0.0025\n",
              "2      1963-09-30 -0.0157 -0.0052  0.0013 -0.0071  0.0029  0.0027\n",
              "3      1963-10-31  0.0253 -0.0139 -0.0010  0.0280 -0.0201  0.0029\n",
              "4      1963-11-30 -0.0085 -0.0088  0.0175 -0.0051  0.0224  0.0027"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ff5 = pd.DataFrame(gff.famaFrench5Factor(frequency='m'))\n",
        "ff5.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "H0GUdoHbzzcm"
      },
      "outputs": [],
      "source": [
        "fff = ['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA']\n",
        "\n",
        "N_lags = 10\n",
        "\n",
        "for f in fff:\n",
        "  for i in range(1, N_lags):\n",
        "    ff5[f'{f}_{i}L'] = ff5[f].shift(-i)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "mon = pd.DataFrame(df[tickers[0]]).resample('ME').last()\n",
        "mon_rets = mon.pct_change().dropna()\n",
        "\n",
        "factors = ff5.rename(columns={'date_ff_factors': 'Date'})\n",
        "factors_0 = pd.merge(mon_rets, factors, on='Date', how='left')\n",
        "factors_0 = factors_0.dropna()\n",
        "\n",
        "Y = (factors_0[tickers[0]] - factors_0['RF'])\n",
        "X = factors_0.drop(\n",
        "    columns=['RF', tickers[0]]).set_index('Date')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "top15features_stable = ['RMW_5L',\n",
        "                        'RMW_2L',\n",
        "                        'SMB_6L',\n",
        "                        'CMA_9L',\n",
        "                        'SMB_2L',\n",
        "                        'HML_5L',\n",
        "                        'Mkt-RF_7L',\n",
        "                        'CMA_4L',\n",
        "                        'CMA_6L',\n",
        "                        'RMW_6L',\n",
        "                        'CMA_8L',\n",
        "                        'Mkt-RF_2L',\n",
        "                        'CMA',\n",
        "                        'RMW',\n",
        "                        'Mkt-RF']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "CCBt1eVkzzco"
      },
      "outputs": [],
      "source": [
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, Y, test_size=0.2, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kk8q9trczzcp"
      },
      "source": [
        "## Обучение LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pn_E8pTUzzcp"
      },
      "source": [
        "#### How to use LSTM for 5-factor Fama French model\n",
        "\n",
        "`https://downloads.hindawi.com/journals/ddns/2022/3936122.pdf?_gl=1*8t8631*_ga*MTYwMjYwNDYwNy4xNzE1MzYyMzYy*_ga_NF5QFMJT5V*MTcxNTM2MjM2Mi4xLjEuMTcxNTM2MjQyNC42MC4wLjA.&_ga=2.120926721.845710784.1715362363-1602604607.1715362362`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRmvbM9jzzcp",
        "outputId": "9addc238-1395-4102-922c-87c5b004626c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ahM2_MQDzzcq"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "# Определение размерности входных данных\n",
        "input_size = 15  # Размерность входного слоя (например, количество признаков)\n",
        "hidden_size = 10  # Размерность скрытого слоя\n",
        "num_layers = 10  # Количество слоев LSTM\n",
        "# Размерность выходного слоя (например, количество прогнозируемых значений)\n",
        "output_size = 1\n",
        "\n",
        "# Определение модели LSTM\n",
        "\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size,\n",
        "                            num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)  # Инициализация скрытого состояния\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)  # Инициализация ячеек состояния\n",
        "        out, _ = self.lstm(x, (h0, c0))  # Прямое распространение через LSTM\n",
        "        # Применение полносвязного слоя к последнему тензору времени\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "mVvJDXlgzzcq"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Создание экземпляра модели\n",
        "model = LSTMModel(input_size, hidden_size, num_layers, output_size)\n",
        "\n",
        "# Определение функции потерь и оптимизатора\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "GQwib5lxzzcq"
      },
      "outputs": [],
      "source": [
        "# train tensors\n",
        "X_train_tensor = torch.tensor(\n",
        "    X_train.loc[:, top15features_stable].values, dtype=torch.float32)\n",
        "X_train_tensor = X_train_tensor.unsqueeze(1)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
        "\n",
        "\n",
        "# test tensors\n",
        "X_test_tensor = torch.tensor(\n",
        "    X_test.loc[:, top15features_stable].values, dtype=torch.float32)\n",
        "X_test_tensor = X_test_tensor.unsqueeze(1)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
        "\n",
        "model.to(device)\n",
        "X_train_tensor = X_train_tensor.to(device)\n",
        "y_train_tensor = y_train_tensor.to(device)\n",
        "\n",
        "X_test_tensor = X_test_tensor.to(device)\n",
        "y_test_tensor = y_test_tensor.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMPvzz2Qzzcq",
        "outputId": "67f8923f-7b32-45b5-eef1-303c103a9370"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/1000 [00:00<?, ?it/s]/Users/daniel/Desktop/Projects/DS/cc/Fama-French-models/.venv/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "  3%|▎         | 28/1000 [00:00<00:06, 149.40it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 0.01596958190202713\n",
            "Epoch 2, Loss: 0.015443412587046623\n",
            "Epoch 3, Loss: 0.01493040844798088\n",
            "Epoch 4, Loss: 0.014430873095989227\n",
            "Epoch 5, Loss: 0.013945050537586212\n",
            "Epoch 6, Loss: 0.013473118655383587\n",
            "Epoch 7, Loss: 0.013015287928283215\n",
            "Epoch 8, Loss: 0.01257181353867054\n",
            "Epoch 9, Loss: 0.012142952531576157\n",
            "Epoch 10, Loss: 0.011728955432772636\n",
            "Epoch 11, Loss: 0.01133005227893591\n",
            "Epoch 12, Loss: 0.01094646006822586\n",
            "Epoch 13, Loss: 0.010578353889286518\n",
            "Epoch 14, Loss: 0.01022589486092329\n",
            "Epoch 15, Loss: 0.009889213368296623\n",
            "Epoch 16, Loss: 0.009568408131599426\n",
            "Epoch 17, Loss: 0.009263547137379646\n",
            "Epoch 18, Loss: 0.008974662981927395\n",
            "Epoch 19, Loss: 0.008701753802597523\n",
            "Epoch 20, Loss: 0.008444778621196747\n",
            "Epoch 21, Loss: 0.008203661069273949\n",
            "Epoch 22, Loss: 0.007978281937539577\n",
            "Epoch 23, Loss: 0.0077684782445430756\n",
            "Epoch 24, Loss: 0.007574041374027729\n",
            "Epoch 25, Loss: 0.007394722197204828\n",
            "Epoch 26, Loss: 0.007230218965560198\n",
            "Epoch 27, Loss: 0.007080181036144495\n",
            "Epoch 28, Loss: 0.0069442084059119225\n",
            "Epoch 29, Loss: 0.006821855437010527\n",
            "Epoch 30, Loss: 0.006712619215250015\n",
            "Epoch 31, Loss: 0.006615947932004929\n",
            "Epoch 32, Loss: 0.006531247869133949\n",
            "Epoch 33, Loss: 0.00645786989480257\n",
            "Epoch 34, Loss: 0.006395129486918449\n",
            "Epoch 35, Loss: 0.006342296022921801\n",
            "Epoch 36, Loss: 0.0062986109405756\n",
            "Epoch 37, Loss: 0.006263281684368849\n",
            "Epoch 38, Loss: 0.006235499866306782\n",
            "Epoch 39, Loss: 0.006214442662894726\n",
            "Epoch 40, Loss: 0.006199283059686422\n",
            "Epoch 41, Loss: 0.006189203355461359\n",
            "Epoch 42, Loss: 0.0061834026128053665\n",
            "Epoch 43, Loss: 0.006181107368320227\n",
            "Epoch 44, Loss: 0.006181586068123579\n",
            "Epoch 45, Loss: 0.006184154190123081\n",
            "Epoch 46, Loss: 0.006188186816871166\n",
            "Epoch 47, Loss: 0.006193125154823065\n",
            "Epoch 48, Loss: 0.006198482122272253\n",
            "Epoch 49, Loss: 0.006203848868608475\n",
            "Epoch 50, Loss: 0.006208889652043581\n",
            "Epoch 51, Loss: 0.006213350221514702\n",
            "Epoch 52, Loss: 0.006217043846845627\n",
            "Epoch 53, Loss: 0.00621985737234354\n",
            "Epoch 54, Loss: 0.006221733521670103\n",
            "Epoch 55, Loss: 0.006222671829164028\n",
            "Epoch 56, Loss: 0.006222711876034737\n",
            "Epoch 57, Loss: 0.006221931427717209\n",
            "Epoch 58, Loss: 0.006220430601388216\n",
            "Epoch 59, Loss: 0.006218326278030872\n",
            "Epoch 60, Loss: 0.0062157451175153255\n",
            "Epoch 61, Loss: 0.00621281610801816\n",
            "Epoch 62, Loss: 0.0062096621841192245\n",
            "Epoch 63, Loss: 0.0062063997611403465\n",
            "Epoch 64, Loss: 0.006203133147209883\n",
            "Epoch 65, Loss: 0.006199950352311134\n",
            "Epoch 66, Loss: 0.006196925882250071\n",
            "Epoch 67, Loss: 0.006194117479026318\n",
            "Epoch 68, Loss: 0.006191567052155733\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 12%|█▏        | 120/1000 [00:00<00:02, 354.62it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 69, Loss: 0.0061893039382994175\n",
            "Epoch 70, Loss: 0.006187339313328266\n",
            "Epoch 71, Loss: 0.006185675971210003\n",
            "Epoch 72, Loss: 0.006184306927025318\n",
            "Epoch 73, Loss: 0.006183214019984007\n",
            "Epoch 74, Loss: 0.00618237629532814\n",
            "Epoch 75, Loss: 0.006181766744703054\n",
            "Epoch 76, Loss: 0.006181355100125074\n",
            "Epoch 77, Loss: 0.006181110627949238\n",
            "Epoch 78, Loss: 0.006181003525853157\n",
            "Epoch 79, Loss: 0.00618100306019187\n",
            "Epoch 80, Loss: 0.006181081756949425\n",
            "Epoch 81, Loss: 0.00618121400475502\n",
            "Epoch 82, Loss: 0.006181378848850727\n",
            "Epoch 83, Loss: 0.006181556731462479\n",
            "Epoch 84, Loss: 0.006181732285767794\n",
            "Epoch 85, Loss: 0.006181892938911915\n",
            "Epoch 86, Loss: 0.006182030308991671\n",
            "Epoch 87, Loss: 0.006182139739394188\n",
            "Epoch 88, Loss: 0.006182217504829168\n",
            "Epoch 89, Loss: 0.006182260811328888\n",
            "Epoch 90, Loss: 0.006182271987199783\n",
            "Epoch 91, Loss: 0.006182252895087004\n",
            "Epoch 92, Loss: 0.006182207725942135\n",
            "Epoch 93, Loss: 0.006182139739394188\n",
            "Epoch 94, Loss: 0.006182054989039898\n",
            "Epoch 95, Loss: 0.006181956734508276\n",
            "Epoch 96, Loss: 0.006181851029396057\n",
            "Epoch 97, Loss: 0.0061817411333322525\n",
            "Epoch 98, Loss: 0.006181631237268448\n",
            "Epoch 99, Loss: 0.006181526463478804\n",
            "Epoch 100, Loss: 0.006181427277624607\n",
            "Epoch 101, Loss: 0.006181336939334869\n",
            "Epoch 102, Loss: 0.00618125731125474\n",
            "Epoch 103, Loss: 0.006181187927722931\n",
            "Epoch 104, Loss: 0.0061811297200620174\n",
            "Epoch 105, Loss: 0.006181084085255861\n",
            "Epoch 106, Loss: 0.006181049160659313\n",
            "Epoch 107, Loss: 0.006181023083627224\n",
            "Epoch 108, Loss: 0.006181005388498306\n",
            "Epoch 109, Loss: 0.00618099607527256\n",
            "Epoch 110, Loss: 0.006180991884320974\n",
            "Epoch 111, Loss: 0.006180992815643549\n",
            "Epoch 112, Loss: 0.006180997006595135\n",
            "Epoch 113, Loss: 0.006181003525853157\n",
            "Epoch 114, Loss: 0.006181011442095041\n",
            "Epoch 115, Loss: 0.0061810193583369255\n",
            "Epoch 116, Loss: 0.006181026808917522\n",
            "Epoch 117, Loss: 0.006181033793836832\n",
            "Epoch 118, Loss: 0.00618103938177228\n",
            "Epoch 119, Loss: 0.006181043107062578\n",
            "Epoch 120, Loss: 0.006181044969707727\n",
            "Epoch 121, Loss: 0.006181046366691589\n",
            "Epoch 122, Loss: 0.00618104450404644\n",
            "Epoch 123, Loss: 0.006181042641401291\n",
            "Epoch 124, Loss: 0.006181039847433567\n",
            "Epoch 125, Loss: 0.006181035190820694\n",
            "Epoch 126, Loss: 0.006181030534207821\n",
            "Epoch 127, Loss: 0.006181025877594948\n",
            "Epoch 128, Loss: 0.006181019823998213\n",
            "Epoch 129, Loss: 0.00618101516738534\n",
            "Epoch 130, Loss: 0.006181010510772467\n",
            "Epoch 131, Loss: 0.006181006785482168\n",
            "Epoch 132, Loss: 0.006181002594530582\n",
            "Epoch 133, Loss: 0.006180999800562859\n",
            "Epoch 134, Loss: 0.006180997006595135\n",
            "Epoch 135, Loss: 0.006180994678288698\n",
            "Epoch 136, Loss: 0.006180994212627411\n",
            "Epoch 137, Loss: 0.006180992349982262\n",
            "Epoch 138, Loss: 0.006180991884320974\n",
            "Epoch 139, Loss: 0.006180991884320974\n",
            "Epoch 140, Loss: 0.006180991884320974\n",
            "Epoch 141, Loss: 0.006180991418659687\n",
            "Epoch 142, Loss: 0.006180992349982262\n",
            "Epoch 143, Loss: 0.006180992815643549\n",
            "Epoch 144, Loss: 0.006180992815643549\n",
            "Epoch 145, Loss: 0.006180993746966124\n",
            "Epoch 146, Loss: 0.006180994212627411\n",
            "Epoch 147, Loss: 0.006180994678288698\n",
            "Epoch 148, Loss: 0.006180994678288698\n",
            "Epoch 149, Loss: 0.006180994212627411\n",
            "Epoch 150, Loss: 0.0061809951439499855\n",
            "Epoch 151, Loss: 0.006180994212627411\n",
            "Epoch 152, Loss: 0.006180994678288698\n",
            "Epoch 153, Loss: 0.006180993746966124\n",
            "Epoch 154, Loss: 0.006180994678288698\n",
            "Epoch 155, Loss: 0.006180992815643549\n",
            "Epoch 156, Loss: 0.006180992815643549\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 21%|██        | 208/1000 [00:00<00:01, 407.62it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 157, Loss: 0.006180992815643549\n",
            "Epoch 158, Loss: 0.006180992815643549\n",
            "Epoch 159, Loss: 0.006180992349982262\n",
            "Epoch 160, Loss: 0.006180991884320974\n",
            "Epoch 161, Loss: 0.006180992349982262\n",
            "Epoch 162, Loss: 0.006180991418659687\n",
            "Epoch 163, Loss: 0.006180991418659687\n",
            "Epoch 164, Loss: 0.006180991884320974\n",
            "Epoch 165, Loss: 0.006180991418659687\n",
            "Epoch 166, Loss: 0.006180991884320974\n",
            "Epoch 167, Loss: 0.006180991418659687\n",
            "Epoch 168, Loss: 0.006180992349982262\n",
            "Epoch 169, Loss: 0.006180992349982262\n",
            "Epoch 170, Loss: 0.006180991418659687\n",
            "Epoch 171, Loss: 0.006180992349982262\n",
            "Epoch 172, Loss: 0.006180992349982262\n",
            "Epoch 173, Loss: 0.006180991418659687\n",
            "Epoch 174, Loss: 0.006180992349982262\n",
            "Epoch 175, Loss: 0.006180991884320974\n",
            "Epoch 176, Loss: 0.006180991884320974\n",
            "Epoch 177, Loss: 0.006180991884320974\n",
            "Epoch 178, Loss: 0.006180991418659687\n",
            "Epoch 179, Loss: 0.006180992349982262\n",
            "Epoch 180, Loss: 0.006180991884320974\n",
            "Epoch 181, Loss: 0.006180992349982262\n",
            "Epoch 182, Loss: 0.006180991884320974\n",
            "Epoch 183, Loss: 0.006180991884320974\n",
            "Epoch 184, Loss: 0.006180991884320974\n",
            "Epoch 185, Loss: 0.006180991418659687\n",
            "Epoch 186, Loss: 0.006180991884320974\n",
            "Epoch 187, Loss: 0.006180992349982262\n",
            "Epoch 188, Loss: 0.006180991884320974\n",
            "Epoch 189, Loss: 0.006180992349982262\n",
            "Epoch 190, Loss: 0.006180991418659687\n",
            "Epoch 191, Loss: 0.006180991884320974\n",
            "Epoch 192, Loss: 0.006180991418659687\n",
            "Epoch 193, Loss: 0.006180991884320974\n",
            "Epoch 194, Loss: 0.006180991884320974\n",
            "Epoch 195, Loss: 0.006180991418659687\n",
            "Epoch 196, Loss: 0.006180991884320974\n",
            "Epoch 197, Loss: 0.006180991418659687\n",
            "Epoch 198, Loss: 0.006180991418659687\n",
            "Epoch 199, Loss: 0.006180991884320974\n",
            "Epoch 200, Loss: 0.006180992349982262\n",
            "Epoch 201, Loss: 0.006180991418659687\n",
            "Epoch 202, Loss: 0.006180991418659687\n",
            "Epoch 203, Loss: 0.006180991884320974\n",
            "Epoch 204, Loss: 0.006180991418659687\n",
            "Epoch 205, Loss: 0.006180991884320974\n",
            "Epoch 206, Loss: 0.006180991884320974\n",
            "Epoch 207, Loss: 0.006180992349982262\n",
            "Epoch 208, Loss: 0.006180991884320974\n",
            "Epoch 209, Loss: 0.006180991418659687\n",
            "Epoch 210, Loss: 0.006180991418659687\n",
            "Epoch 211, Loss: 0.006180991418659687\n",
            "Epoch 212, Loss: 0.006180991418659687\n",
            "Epoch 213, Loss: 0.006180990487337112\n",
            "Epoch 214, Loss: 0.006180991884320974\n",
            "Epoch 215, Loss: 0.006180991884320974\n",
            "Epoch 216, Loss: 0.006180992349982262\n",
            "Epoch 217, Loss: 0.006180992349982262\n",
            "Epoch 218, Loss: 0.006180991884320974\n",
            "Epoch 219, Loss: 0.006180991418659687\n",
            "Epoch 220, Loss: 0.006180991884320974\n",
            "Epoch 221, Loss: 0.006180991884320974\n",
            "Epoch 222, Loss: 0.006180991884320974\n",
            "Epoch 223, Loss: 0.006180991418659687\n",
            "Epoch 224, Loss: 0.006180991884320974\n",
            "Epoch 225, Loss: 0.006180991884320974\n",
            "Epoch 226, Loss: 0.006180991884320974\n",
            "Epoch 227, Loss: 0.006180991884320974\n",
            "Epoch 228, Loss: 0.006180991884320974\n",
            "Epoch 229, Loss: 0.006180991418659687\n",
            "Epoch 230, Loss: 0.006180992349982262\n",
            "Epoch 231, Loss: 0.006180991418659687\n",
            "Epoch 232, Loss: 0.006180991418659687\n",
            "Epoch 233, Loss: 0.006180991884320974\n",
            "Epoch 234, Loss: 0.006180991884320974\n",
            "Epoch 235, Loss: 0.0061809909529984\n",
            "Epoch 236, Loss: 0.0061809909529984\n",
            "Epoch 237, Loss: 0.006180991418659687\n",
            "Epoch 238, Loss: 0.006180991418659687\n",
            "Epoch 239, Loss: 0.006180991884320974\n",
            "Epoch 240, Loss: 0.006180991884320974\n",
            "Epoch 241, Loss: 0.006180992349982262\n",
            "Epoch 242, Loss: 0.006180991884320974\n",
            "Epoch 243, Loss: 0.006180991884320974\n",
            "Epoch 244, Loss: 0.006180991418659687\n",
            "Epoch 245, Loss: 0.006180992349982262\n",
            "Epoch 246, Loss: 0.006180991418659687\n",
            "Epoch 247, Loss: 0.006180991418659687\n",
            "Epoch 248, Loss: 0.006180991418659687\n",
            "Epoch 249, Loss: 0.006180991418659687\n",
            "Epoch 250, Loss: 0.006180991418659687\n",
            "Epoch 251, Loss: 0.006180991418659687\n",
            "Epoch 252, Loss: 0.006180991418659687\n",
            "Epoch 253, Loss: 0.006180991418659687\n",
            "Epoch 254, Loss: 0.006180991418659687\n",
            "Epoch 255, Loss: 0.006180992349982262\n",
            "Epoch 256, Loss: 0.006180992349982262\n",
            "Epoch 257, Loss: 0.006180991884320974\n",
            "Epoch 258, Loss: 0.006180991418659687\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 36%|███▋      | 363/1000 [00:00<00:01, 481.20it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 259, Loss: 0.006180992349982262\n",
            "Epoch 260, Loss: 0.006180991884320974\n",
            "Epoch 261, Loss: 0.006180991884320974\n",
            "Epoch 262, Loss: 0.006180991418659687\n",
            "Epoch 263, Loss: 0.006180991884320974\n",
            "Epoch 264, Loss: 0.006180991884320974\n",
            "Epoch 265, Loss: 0.006180991418659687\n",
            "Epoch 266, Loss: 0.006180992349982262\n",
            "Epoch 267, Loss: 0.006180992349982262\n",
            "Epoch 268, Loss: 0.006180992349982262\n",
            "Epoch 269, Loss: 0.006180992349982262\n",
            "Epoch 270, Loss: 0.006180992349982262\n",
            "Epoch 271, Loss: 0.006180992349982262\n",
            "Epoch 272, Loss: 0.006180992349982262\n",
            "Epoch 273, Loss: 0.006180992349982262\n",
            "Epoch 274, Loss: 0.006180992349982262\n",
            "Epoch 275, Loss: 0.006180992349982262\n",
            "Epoch 276, Loss: 0.006180992349982262\n",
            "Epoch 277, Loss: 0.006180992349982262\n",
            "Epoch 278, Loss: 0.006180992349982262\n",
            "Epoch 279, Loss: 0.006180992349982262\n",
            "Epoch 280, Loss: 0.006180992349982262\n",
            "Epoch 281, Loss: 0.006180992349982262\n",
            "Epoch 282, Loss: 0.006180992349982262\n",
            "Epoch 283, Loss: 0.006180992349982262\n",
            "Epoch 284, Loss: 0.006180992349982262\n",
            "Epoch 285, Loss: 0.006180992349982262\n",
            "Epoch 286, Loss: 0.006180992349982262\n",
            "Epoch 287, Loss: 0.006180992349982262\n",
            "Epoch 288, Loss: 0.006180992349982262\n",
            "Epoch 289, Loss: 0.006180991884320974\n",
            "Epoch 290, Loss: 0.006180991884320974\n",
            "Epoch 291, Loss: 0.006180991884320974\n",
            "Epoch 292, Loss: 0.006180991884320974\n",
            "Epoch 293, Loss: 0.006180992349982262\n",
            "Epoch 294, Loss: 0.006180992349982262\n",
            "Epoch 295, Loss: 0.006180991884320974\n",
            "Epoch 296, Loss: 0.006180991884320974\n",
            "Epoch 297, Loss: 0.006180991418659687\n",
            "Epoch 298, Loss: 0.006180991884320974\n",
            "Epoch 299, Loss: 0.006180991884320974\n",
            "Epoch 300, Loss: 0.006180991884320974\n",
            "Epoch 301, Loss: 0.006180991884320974\n",
            "Epoch 302, Loss: 0.006180991884320974\n",
            "Epoch 303, Loss: 0.006180991884320974\n",
            "Epoch 304, Loss: 0.006180991884320974\n",
            "Epoch 305, Loss: 0.006180991884320974\n",
            "Epoch 306, Loss: 0.006180991884320974\n",
            "Epoch 307, Loss: 0.006180991884320974\n",
            "Epoch 308, Loss: 0.006180991884320974\n",
            "Epoch 309, Loss: 0.006180991884320974\n",
            "Epoch 310, Loss: 0.006180991884320974\n",
            "Epoch 311, Loss: 0.006180991884320974\n",
            "Epoch 312, Loss: 0.006180991884320974\n",
            "Epoch 313, Loss: 0.006180991884320974\n",
            "Epoch 314, Loss: 0.006180991884320974\n",
            "Epoch 315, Loss: 0.006180991884320974\n",
            "Epoch 316, Loss: 0.006180991884320974\n",
            "Epoch 317, Loss: 0.006180991884320974\n",
            "Epoch 318, Loss: 0.006180991884320974\n",
            "Epoch 319, Loss: 0.006180991884320974\n",
            "Epoch 320, Loss: 0.006180991884320974\n",
            "Epoch 321, Loss: 0.006180991884320974\n",
            "Epoch 322, Loss: 0.006180991884320974\n",
            "Epoch 323, Loss: 0.006180991884320974\n",
            "Epoch 324, Loss: 0.006180991884320974\n",
            "Epoch 325, Loss: 0.006180991884320974\n",
            "Epoch 326, Loss: 0.006180991884320974\n",
            "Epoch 327, Loss: 0.006180991884320974\n",
            "Epoch 328, Loss: 0.006180991884320974\n",
            "Epoch 329, Loss: 0.006180991884320974\n",
            "Epoch 330, Loss: 0.006180991884320974\n",
            "Epoch 331, Loss: 0.006180991884320974\n",
            "Epoch 332, Loss: 0.006180991884320974\n",
            "Epoch 333, Loss: 0.006180991884320974\n",
            "Epoch 334, Loss: 0.006180991884320974\n",
            "Epoch 335, Loss: 0.006180991884320974\n",
            "Epoch 336, Loss: 0.006180991884320974\n",
            "Epoch 337, Loss: 0.006180991884320974\n",
            "Epoch 338, Loss: 0.006180991884320974\n",
            "Epoch 339, Loss: 0.006180991884320974\n",
            "Epoch 340, Loss: 0.006180991884320974\n",
            "Epoch 341, Loss: 0.006180991884320974\n",
            "Epoch 342, Loss: 0.006180991884320974\n",
            "Epoch 343, Loss: 0.006180991884320974\n",
            "Epoch 344, Loss: 0.006180991884320974\n",
            "Epoch 345, Loss: 0.006180991884320974\n",
            "Epoch 346, Loss: 0.006180991884320974\n",
            "Epoch 347, Loss: 0.006180991884320974\n",
            "Epoch 348, Loss: 0.006180991884320974\n",
            "Epoch 349, Loss: 0.006180991884320974\n",
            "Epoch 350, Loss: 0.006180991884320974\n",
            "Epoch 351, Loss: 0.006180991884320974\n",
            "Epoch 352, Loss: 0.006180991884320974\n",
            "Epoch 353, Loss: 0.006180991884320974\n",
            "Epoch 354, Loss: 0.006180991884320974\n",
            "Epoch 355, Loss: 0.006180991884320974\n",
            "Epoch 356, Loss: 0.006180991884320974\n",
            "Epoch 357, Loss: 0.006180991884320974\n",
            "Epoch 358, Loss: 0.006180991884320974\n",
            "Epoch 359, Loss: 0.006180991884320974\n",
            "Epoch 360, Loss: 0.006180991884320974\n",
            "Epoch 361, Loss: 0.006180991884320974\n",
            "Epoch 362, Loss: 0.006180991884320974\n",
            "Epoch 363, Loss: 0.006180991884320974\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 41%|████      | 412/1000 [00:01<00:01, 480.09it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 364, Loss: 0.006180991884320974\n",
            "Epoch 365, Loss: 0.006180991884320974\n",
            "Epoch 366, Loss: 0.006180991884320974\n",
            "Epoch 367, Loss: 0.006180991884320974\n",
            "Epoch 368, Loss: 0.006180991884320974\n",
            "Epoch 369, Loss: 0.006180991884320974\n",
            "Epoch 370, Loss: 0.006180991884320974\n",
            "Epoch 371, Loss: 0.006180991884320974\n",
            "Epoch 372, Loss: 0.006180991884320974\n",
            "Epoch 373, Loss: 0.006180991884320974\n",
            "Epoch 374, Loss: 0.006180991884320974\n",
            "Epoch 375, Loss: 0.006180991884320974\n",
            "Epoch 376, Loss: 0.006180991884320974\n",
            "Epoch 377, Loss: 0.006180991884320974\n",
            "Epoch 378, Loss: 0.006180991884320974\n",
            "Epoch 379, Loss: 0.006180991884320974\n",
            "Epoch 380, Loss: 0.006180991884320974\n",
            "Epoch 381, Loss: 0.006180991884320974\n",
            "Epoch 382, Loss: 0.006180991884320974\n",
            "Epoch 383, Loss: 0.006180991884320974\n",
            "Epoch 384, Loss: 0.006180991884320974\n",
            "Epoch 385, Loss: 0.006180991884320974\n",
            "Epoch 386, Loss: 0.006180991884320974\n",
            "Epoch 387, Loss: 0.006180991884320974\n",
            "Epoch 388, Loss: 0.006180991884320974\n",
            "Epoch 389, Loss: 0.006180991884320974\n",
            "Epoch 390, Loss: 0.006180991884320974\n",
            "Epoch 391, Loss: 0.006180991884320974\n",
            "Epoch 392, Loss: 0.006180991884320974\n",
            "Epoch 393, Loss: 0.006180991884320974\n",
            "Epoch 394, Loss: 0.006180991884320974\n",
            "Epoch 395, Loss: 0.006180991884320974\n",
            "Epoch 396, Loss: 0.006180991884320974\n",
            "Epoch 397, Loss: 0.006180991884320974\n",
            "Epoch 398, Loss: 0.006180991884320974\n",
            "Epoch 399, Loss: 0.006180991884320974\n",
            "Epoch 400, Loss: 0.006180991884320974\n",
            "Epoch 401, Loss: 0.006180991884320974\n",
            "Epoch 402, Loss: 0.006180991884320974\n",
            "Epoch 403, Loss: 0.006180991884320974\n",
            "Epoch 404, Loss: 0.006180991884320974\n",
            "Epoch 405, Loss: 0.006180991884320974\n",
            "Epoch 406, Loss: 0.006180991884320974\n",
            "Epoch 407, Loss: 0.006180991884320974\n",
            "Epoch 408, Loss: 0.006180991884320974\n",
            "Epoch 409, Loss: 0.006180991884320974\n",
            "Epoch 410, Loss: 0.006180991884320974\n",
            "Epoch 411, Loss: 0.006180991884320974\n",
            "Epoch 412, Loss: 0.006180991884320974\n",
            "Epoch 413, Loss: 0.006180991884320974\n",
            "Epoch 414, Loss: 0.006180991884320974\n",
            "Epoch 415, Loss: 0.006180991884320974\n",
            "Epoch 416, Loss: 0.006180991884320974\n",
            "Epoch 417, Loss: 0.006180991884320974\n",
            "Epoch 418, Loss: 0.006180991884320974\n",
            "Epoch 419, Loss: 0.006180991884320974\n",
            "Epoch 420, Loss: 0.006180991884320974\n",
            "Epoch 421, Loss: 0.006180991884320974\n",
            "Epoch 422, Loss: 0.006180991884320974\n",
            "Epoch 423, Loss: 0.006180991884320974\n",
            "Epoch 424, Loss: 0.006180991884320974\n",
            "Epoch 425, Loss: 0.006180991884320974\n",
            "Epoch 426, Loss: 0.006180991884320974\n",
            "Epoch 427, Loss: 0.006180991884320974\n",
            "Epoch 428, Loss: 0.006180991884320974\n",
            "Epoch 429, Loss: 0.006180991884320974\n",
            "Epoch 430, Loss: 0.006180991884320974\n",
            "Epoch 431, Loss: 0.006180991884320974\n",
            "Epoch 432, Loss: 0.006180991884320974\n",
            "Epoch 433, Loss: 0.006180991884320974\n",
            "Epoch 434, Loss: 0.006180991884320974\n",
            "Epoch 435, Loss: 0.006180991884320974\n",
            "Epoch 436, Loss: 0.006180991884320974\n",
            "Epoch 437, Loss: 0.006180991884320974\n",
            "Epoch 438, Loss: 0.006180991884320974\n",
            "Epoch 439, Loss: 0.006180991884320974\n",
            "Epoch 440, Loss: 0.006180991884320974\n",
            "Epoch 441, Loss: 0.006180991884320974\n",
            "Epoch 442, Loss: 0.006180991884320974\n",
            "Epoch 443, Loss: 0.006180991884320974\n",
            "Epoch 444, Loss: 0.006180991884320974\n",
            "Epoch 445, Loss: 0.006180991884320974\n",
            "Epoch 446, Loss: 0.006180991884320974\n",
            "Epoch 447, Loss: 0.006180991884320974\n",
            "Epoch 448, Loss: 0.006180991884320974\n",
            "Epoch 449, Loss: 0.006180991884320974\n",
            "Epoch 450, Loss: 0.006180991884320974\n",
            "Epoch 451, Loss: 0.006180991884320974\n",
            "Epoch 452, Loss: 0.006180991884320974\n",
            "Epoch 453, Loss: 0.006180991884320974\n",
            "Epoch 454, Loss: 0.006180991884320974\n",
            "Epoch 455, Loss: 0.006180991884320974\n",
            "Epoch 456, Loss: 0.006180991884320974\n",
            "Epoch 457, Loss: 0.006180991884320974\n",
            "Epoch 458, Loss: 0.006180991884320974\n",
            "Epoch 459, Loss: 0.006180991884320974\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 51%|█████     | 510/1000 [00:01<00:01, 480.99it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 460, Loss: 0.006180991884320974\n",
            "Epoch 461, Loss: 0.006180991884320974\n",
            "Epoch 462, Loss: 0.006180991884320974\n",
            "Epoch 463, Loss: 0.006180991884320974\n",
            "Epoch 464, Loss: 0.006180991884320974\n",
            "Epoch 465, Loss: 0.006180991884320974\n",
            "Epoch 466, Loss: 0.006180991884320974\n",
            "Epoch 467, Loss: 0.006180991884320974\n",
            "Epoch 468, Loss: 0.006180991884320974\n",
            "Epoch 469, Loss: 0.006180991884320974\n",
            "Epoch 470, Loss: 0.006180991884320974\n",
            "Epoch 471, Loss: 0.006180991884320974\n",
            "Epoch 472, Loss: 0.006180991884320974\n",
            "Epoch 473, Loss: 0.006180991884320974\n",
            "Epoch 474, Loss: 0.006180991884320974\n",
            "Epoch 475, Loss: 0.006180991884320974\n",
            "Epoch 476, Loss: 0.006180991884320974\n",
            "Epoch 477, Loss: 0.006180991884320974\n",
            "Epoch 478, Loss: 0.006180991884320974\n",
            "Epoch 479, Loss: 0.006180991884320974\n",
            "Epoch 480, Loss: 0.006180991884320974\n",
            "Epoch 481, Loss: 0.006180991884320974\n",
            "Epoch 482, Loss: 0.006180991884320974\n",
            "Epoch 483, Loss: 0.006180991884320974\n",
            "Epoch 484, Loss: 0.006180991884320974\n",
            "Epoch 485, Loss: 0.006180991884320974\n",
            "Epoch 486, Loss: 0.006180991884320974\n",
            "Epoch 487, Loss: 0.006180991884320974\n",
            "Epoch 488, Loss: 0.006180991884320974\n",
            "Epoch 489, Loss: 0.006180991884320974\n",
            "Epoch 490, Loss: 0.006180991884320974\n",
            "Epoch 491, Loss: 0.006180991884320974\n",
            "Epoch 492, Loss: 0.006180991884320974\n",
            "Epoch 493, Loss: 0.006180991884320974\n",
            "Epoch 494, Loss: 0.006180991884320974\n",
            "Epoch 495, Loss: 0.006180991884320974\n",
            "Epoch 496, Loss: 0.006180991884320974\n",
            "Epoch 497, Loss: 0.006180991884320974\n",
            "Epoch 498, Loss: 0.006180991884320974\n",
            "Epoch 499, Loss: 0.006180991884320974\n",
            "Epoch 500, Loss: 0.006180991884320974\n",
            "Epoch 501, Loss: 0.006180991884320974\n",
            "Epoch 502, Loss: 0.006180991884320974\n",
            "Epoch 503, Loss: 0.006180991884320974\n",
            "Epoch 504, Loss: 0.006180991884320974\n",
            "Epoch 505, Loss: 0.006180991884320974\n",
            "Epoch 506, Loss: 0.006180991884320974\n",
            "Epoch 507, Loss: 0.006180991884320974\n",
            "Epoch 508, Loss: 0.006180991884320974\n",
            "Epoch 509, Loss: 0.006180991884320974\n",
            "Epoch 510, Loss: 0.006180991884320974\n",
            "Epoch 511, Loss: 0.006180991884320974\n",
            "Epoch 512, Loss: 0.006180991884320974\n",
            "Epoch 513, Loss: 0.006180991884320974\n",
            "Epoch 514, Loss: 0.006180991884320974\n",
            "Epoch 515, Loss: 0.006180991884320974\n",
            "Epoch 516, Loss: 0.006180991884320974\n",
            "Epoch 517, Loss: 0.006180991884320974\n",
            "Epoch 518, Loss: 0.006180991884320974\n",
            "Epoch 519, Loss: 0.006180991884320974\n",
            "Epoch 520, Loss: 0.006180991884320974\n",
            "Epoch 521, Loss: 0.006180991884320974\n",
            "Epoch 522, Loss: 0.006180991884320974\n",
            "Epoch 523, Loss: 0.006180991884320974\n",
            "Epoch 524, Loss: 0.006180991884320974\n",
            "Epoch 525, Loss: 0.006180991884320974\n",
            "Epoch 526, Loss: 0.006180991884320974\n",
            "Epoch 527, Loss: 0.006180991884320974\n",
            "Epoch 528, Loss: 0.006180991884320974\n",
            "Epoch 529, Loss: 0.006180991884320974\n",
            "Epoch 530, Loss: 0.006180991884320974\n",
            "Epoch 531, Loss: 0.006180991884320974\n",
            "Epoch 532, Loss: 0.006180991884320974\n",
            "Epoch 533, Loss: 0.006180991884320974\n",
            "Epoch 534, Loss: 0.006180991884320974\n",
            "Epoch 535, Loss: 0.006180991884320974\n",
            "Epoch 536, Loss: 0.006180991884320974\n",
            "Epoch 537, Loss: 0.006180991884320974\n",
            "Epoch 538, Loss: 0.006180991884320974\n",
            "Epoch 539, Loss: 0.006180991884320974\n",
            "Epoch 540, Loss: 0.006180991884320974\n",
            "Epoch 541, Loss: 0.006180991884320974\n",
            "Epoch 542, Loss: 0.006180991884320974\n",
            "Epoch 543, Loss: 0.006180991884320974\n",
            "Epoch 544, Loss: 0.006180991884320974\n",
            "Epoch 545, Loss: 0.006180991884320974\n",
            "Epoch 546, Loss: 0.006180991884320974\n",
            "Epoch 547, Loss: 0.006180991884320974\n",
            "Epoch 548, Loss: 0.006180991884320974\n",
            "Epoch 549, Loss: 0.006180991884320974\n",
            "Epoch 550, Loss: 0.006180991884320974\n",
            "Epoch 551, Loss: 0.006180991884320974\n",
            "Epoch 552, Loss: 0.006180991884320974\n",
            "Epoch 553, Loss: 0.006180991884320974\n",
            "Epoch 554, Loss: 0.006180991884320974\n",
            "Epoch 555, Loss: 0.006180991884320974\n",
            "Epoch 556, Loss: 0.006180991884320974\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 61%|██████    | 608/1000 [00:01<00:00, 480.86it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 557, Loss: 0.006180991884320974\n",
            "Epoch 558, Loss: 0.006180991884320974\n",
            "Epoch 559, Loss: 0.006180991884320974\n",
            "Epoch 560, Loss: 0.006180991884320974\n",
            "Epoch 561, Loss: 0.006180991884320974\n",
            "Epoch 562, Loss: 0.006180991884320974\n",
            "Epoch 563, Loss: 0.006180991884320974\n",
            "Epoch 564, Loss: 0.006180991884320974\n",
            "Epoch 565, Loss: 0.006180991884320974\n",
            "Epoch 566, Loss: 0.006180991884320974\n",
            "Epoch 567, Loss: 0.006180991884320974\n",
            "Epoch 568, Loss: 0.006180991884320974\n",
            "Epoch 569, Loss: 0.006180991884320974\n",
            "Epoch 570, Loss: 0.006180991884320974\n",
            "Epoch 571, Loss: 0.006180991884320974\n",
            "Epoch 572, Loss: 0.006180991884320974\n",
            "Epoch 573, Loss: 0.006180991884320974\n",
            "Epoch 574, Loss: 0.006180991884320974\n",
            "Epoch 575, Loss: 0.006180991884320974\n",
            "Epoch 576, Loss: 0.006180991884320974\n",
            "Epoch 577, Loss: 0.006180991884320974\n",
            "Epoch 578, Loss: 0.006180991884320974\n",
            "Epoch 579, Loss: 0.006180991884320974\n",
            "Epoch 580, Loss: 0.006180991884320974\n",
            "Epoch 581, Loss: 0.006180991884320974\n",
            "Epoch 582, Loss: 0.006180991884320974\n",
            "Epoch 583, Loss: 0.006180991884320974\n",
            "Epoch 584, Loss: 0.006180991884320974\n",
            "Epoch 585, Loss: 0.006180991884320974\n",
            "Epoch 586, Loss: 0.006180991884320974\n",
            "Epoch 587, Loss: 0.006180991884320974\n",
            "Epoch 588, Loss: 0.006180991884320974\n",
            "Epoch 589, Loss: 0.006180991884320974\n",
            "Epoch 590, Loss: 0.006180991884320974\n",
            "Epoch 591, Loss: 0.006180991884320974\n",
            "Epoch 592, Loss: 0.006180991884320974\n",
            "Epoch 593, Loss: 0.006180991884320974\n",
            "Epoch 594, Loss: 0.006180991884320974\n",
            "Epoch 595, Loss: 0.006180991884320974\n",
            "Epoch 596, Loss: 0.006180991884320974\n",
            "Epoch 597, Loss: 0.006180991884320974\n",
            "Epoch 598, Loss: 0.006180991884320974\n",
            "Epoch 599, Loss: 0.006180991884320974\n",
            "Epoch 600, Loss: 0.006180991884320974\n",
            "Epoch 601, Loss: 0.006180991884320974\n",
            "Epoch 602, Loss: 0.006180991884320974\n",
            "Epoch 603, Loss: 0.006180991884320974\n",
            "Epoch 604, Loss: 0.006180991884320974\n",
            "Epoch 605, Loss: 0.006180991884320974\n",
            "Epoch 606, Loss: 0.006180991884320974\n",
            "Epoch 607, Loss: 0.006180991884320974\n",
            "Epoch 608, Loss: 0.006180991884320974\n",
            "Epoch 609, Loss: 0.006180991884320974\n",
            "Epoch 610, Loss: 0.006180991884320974\n",
            "Epoch 611, Loss: 0.006180991884320974\n",
            "Epoch 612, Loss: 0.006180991884320974\n",
            "Epoch 613, Loss: 0.006180991884320974\n",
            "Epoch 614, Loss: 0.006180991884320974\n",
            "Epoch 615, Loss: 0.006180991884320974\n",
            "Epoch 616, Loss: 0.006180991884320974\n",
            "Epoch 617, Loss: 0.006180991884320974\n",
            "Epoch 618, Loss: 0.006180991884320974\n",
            "Epoch 619, Loss: 0.006180991884320974\n",
            "Epoch 620, Loss: 0.006180991884320974\n",
            "Epoch 621, Loss: 0.006180991884320974\n",
            "Epoch 622, Loss: 0.006180991884320974\n",
            "Epoch 623, Loss: 0.006180991884320974\n",
            "Epoch 624, Loss: 0.006180991884320974\n",
            "Epoch 625, Loss: 0.006180991884320974\n",
            "Epoch 626, Loss: 0.006180991884320974\n",
            "Epoch 627, Loss: 0.006180991884320974\n",
            "Epoch 628, Loss: 0.006180991884320974\n",
            "Epoch 629, Loss: 0.006180991884320974\n",
            "Epoch 630, Loss: 0.006180991884320974\n",
            "Epoch 631, Loss: 0.006180991884320974\n",
            "Epoch 632, Loss: 0.006180991884320974\n",
            "Epoch 633, Loss: 0.006180991884320974\n",
            "Epoch 634, Loss: 0.006180991884320974\n",
            "Epoch 635, Loss: 0.006180991884320974\n",
            "Epoch 636, Loss: 0.006180991884320974\n",
            "Epoch 637, Loss: 0.006180991884320974\n",
            "Epoch 638, Loss: 0.006180991884320974\n",
            "Epoch 639, Loss: 0.006180991884320974\n",
            "Epoch 640, Loss: 0.006180991884320974\n",
            "Epoch 641, Loss: 0.006180991884320974\n",
            "Epoch 642, Loss: 0.006180991884320974\n",
            "Epoch 643, Loss: 0.006180991884320974\n",
            "Epoch 644, Loss: 0.006180991884320974\n",
            "Epoch 645, Loss: 0.006180991884320974\n",
            "Epoch 646, Loss: 0.006180991884320974\n",
            "Epoch 647, Loss: 0.006180991884320974\n",
            "Epoch 648, Loss: 0.006180991884320974\n",
            "Epoch 649, Loss: 0.006180991884320974\n",
            "Epoch 650, Loss: 0.006180991884320974\n",
            "Epoch 651, Loss: 0.006180991884320974\n",
            "Epoch 652, Loss: 0.006180991884320974\n",
            "Epoch 653, Loss: 0.006180991884320974\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 71%|███████   | 708/1000 [00:01<00:00, 483.65it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 654, Loss: 0.006180991884320974\n",
            "Epoch 655, Loss: 0.006180991884320974\n",
            "Epoch 656, Loss: 0.006180991884320974\n",
            "Epoch 657, Loss: 0.006180991884320974\n",
            "Epoch 658, Loss: 0.006180991884320974\n",
            "Epoch 659, Loss: 0.006180991884320974\n",
            "Epoch 660, Loss: 0.006180991884320974\n",
            "Epoch 661, Loss: 0.006180991884320974\n",
            "Epoch 662, Loss: 0.006180991884320974\n",
            "Epoch 663, Loss: 0.006180991884320974\n",
            "Epoch 664, Loss: 0.006180991884320974\n",
            "Epoch 665, Loss: 0.006180991884320974\n",
            "Epoch 666, Loss: 0.006180991884320974\n",
            "Epoch 667, Loss: 0.006180991884320974\n",
            "Epoch 668, Loss: 0.006180991884320974\n",
            "Epoch 669, Loss: 0.006180991884320974\n",
            "Epoch 670, Loss: 0.006180991884320974\n",
            "Epoch 671, Loss: 0.006180991884320974\n",
            "Epoch 672, Loss: 0.006180991884320974\n",
            "Epoch 673, Loss: 0.006180991884320974\n",
            "Epoch 674, Loss: 0.006180991884320974\n",
            "Epoch 675, Loss: 0.006180991884320974\n",
            "Epoch 676, Loss: 0.006180991884320974\n",
            "Epoch 677, Loss: 0.006180991884320974\n",
            "Epoch 678, Loss: 0.006180991884320974\n",
            "Epoch 679, Loss: 0.006180991884320974\n",
            "Epoch 680, Loss: 0.006180991884320974\n",
            "Epoch 681, Loss: 0.006180991884320974\n",
            "Epoch 682, Loss: 0.006180991884320974\n",
            "Epoch 683, Loss: 0.006180991884320974\n",
            "Epoch 684, Loss: 0.006180991884320974\n",
            "Epoch 685, Loss: 0.006180991884320974\n",
            "Epoch 686, Loss: 0.006180991884320974\n",
            "Epoch 687, Loss: 0.006180991884320974\n",
            "Epoch 688, Loss: 0.006180991884320974\n",
            "Epoch 689, Loss: 0.006180991884320974\n",
            "Epoch 690, Loss: 0.006180991884320974\n",
            "Epoch 691, Loss: 0.006180991884320974\n",
            "Epoch 692, Loss: 0.006180991884320974\n",
            "Epoch 693, Loss: 0.006180991884320974\n",
            "Epoch 694, Loss: 0.006180991884320974\n",
            "Epoch 695, Loss: 0.006180991884320974\n",
            "Epoch 696, Loss: 0.006180991884320974\n",
            "Epoch 697, Loss: 0.006180991884320974\n",
            "Epoch 698, Loss: 0.006180991884320974\n",
            "Epoch 699, Loss: 0.006180991884320974\n",
            "Epoch 700, Loss: 0.006180991884320974\n",
            "Epoch 701, Loss: 0.006180991884320974\n",
            "Epoch 702, Loss: 0.006180991884320974\n",
            "Epoch 703, Loss: 0.006180991884320974\n",
            "Epoch 704, Loss: 0.006180991884320974\n",
            "Epoch 705, Loss: 0.006180991884320974\n",
            "Epoch 706, Loss: 0.006180991884320974\n",
            "Epoch 707, Loss: 0.006180991884320974\n",
            "Epoch 708, Loss: 0.006180991884320974\n",
            "Epoch 709, Loss: 0.006180991884320974\n",
            "Epoch 710, Loss: 0.006180991884320974\n",
            "Epoch 711, Loss: 0.006180991884320974\n",
            "Epoch 712, Loss: 0.006180991884320974\n",
            "Epoch 713, Loss: 0.006180991884320974\n",
            "Epoch 714, Loss: 0.006180991884320974\n",
            "Epoch 715, Loss: 0.006180991884320974\n",
            "Epoch 716, Loss: 0.006180991884320974\n",
            "Epoch 717, Loss: 0.006180991884320974\n",
            "Epoch 718, Loss: 0.006180991884320974\n",
            "Epoch 719, Loss: 0.006180991884320974\n",
            "Epoch 720, Loss: 0.006180991884320974\n",
            "Epoch 721, Loss: 0.006180991884320974\n",
            "Epoch 722, Loss: 0.006180991884320974\n",
            "Epoch 723, Loss: 0.006180991884320974\n",
            "Epoch 724, Loss: 0.006180991884320974\n",
            "Epoch 725, Loss: 0.006180991884320974\n",
            "Epoch 726, Loss: 0.006180991884320974\n",
            "Epoch 727, Loss: 0.006180991884320974\n",
            "Epoch 728, Loss: 0.006180991884320974\n",
            "Epoch 729, Loss: 0.006180991884320974\n",
            "Epoch 730, Loss: 0.006180991884320974\n",
            "Epoch 731, Loss: 0.006180991884320974\n",
            "Epoch 732, Loss: 0.006180991884320974\n",
            "Epoch 733, Loss: 0.006180991884320974\n",
            "Epoch 734, Loss: 0.006180991884320974\n",
            "Epoch 735, Loss: 0.006180991884320974\n",
            "Epoch 736, Loss: 0.006180991884320974\n",
            "Epoch 737, Loss: 0.006180991884320974\n",
            "Epoch 738, Loss: 0.006180991884320974\n",
            "Epoch 739, Loss: 0.006180991884320974\n",
            "Epoch 740, Loss: 0.006180991884320974\n",
            "Epoch 741, Loss: 0.006180991884320974\n",
            "Epoch 742, Loss: 0.006180991884320974\n",
            "Epoch 743, Loss: 0.006180991884320974\n",
            "Epoch 744, Loss: 0.006180991884320974\n",
            "Epoch 745, Loss: 0.006180991884320974\n",
            "Epoch 746, Loss: 0.006180991884320974\n",
            "Epoch 747, Loss: 0.006180991884320974\n",
            "Epoch 748, Loss: 0.006180991884320974\n",
            "Epoch 749, Loss: 0.006180991884320974\n",
            "Epoch 750, Loss: 0.006180991884320974\n",
            "Epoch 751, Loss: 0.006180991884320974\n",
            "Epoch 752, Loss: 0.006180991884320974\n",
            "Epoch 753, Loss: 0.006180991884320974\n",
            "Epoch 754, Loss: 0.006180991884320974\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 81%|████████  | 808/1000 [00:01<00:00, 488.41it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 755, Loss: 0.006180991884320974\n",
            "Epoch 756, Loss: 0.006180991884320974\n",
            "Epoch 757, Loss: 0.006180991884320974\n",
            "Epoch 758, Loss: 0.006180991884320974\n",
            "Epoch 759, Loss: 0.006180991884320974\n",
            "Epoch 760, Loss: 0.006180991884320974\n",
            "Epoch 761, Loss: 0.006180991884320974\n",
            "Epoch 762, Loss: 0.006180991884320974\n",
            "Epoch 763, Loss: 0.006180991884320974\n",
            "Epoch 764, Loss: 0.006180991884320974\n",
            "Epoch 765, Loss: 0.006180991884320974\n",
            "Epoch 766, Loss: 0.006180991884320974\n",
            "Epoch 767, Loss: 0.006180991884320974\n",
            "Epoch 768, Loss: 0.006180991884320974\n",
            "Epoch 769, Loss: 0.006180991884320974\n",
            "Epoch 770, Loss: 0.006180991884320974\n",
            "Epoch 771, Loss: 0.006180991884320974\n",
            "Epoch 772, Loss: 0.006180991884320974\n",
            "Epoch 773, Loss: 0.006180991884320974\n",
            "Epoch 774, Loss: 0.006180991884320974\n",
            "Epoch 775, Loss: 0.006180991884320974\n",
            "Epoch 776, Loss: 0.006180991884320974\n",
            "Epoch 777, Loss: 0.006180991884320974\n",
            "Epoch 778, Loss: 0.006180991884320974\n",
            "Epoch 779, Loss: 0.006180991884320974\n",
            "Epoch 780, Loss: 0.006180991884320974\n",
            "Epoch 781, Loss: 0.006180991884320974\n",
            "Epoch 782, Loss: 0.006180991884320974\n",
            "Epoch 783, Loss: 0.006180991884320974\n",
            "Epoch 784, Loss: 0.006180991884320974\n",
            "Epoch 785, Loss: 0.006180991884320974\n",
            "Epoch 786, Loss: 0.006180991884320974\n",
            "Epoch 787, Loss: 0.006180991884320974\n",
            "Epoch 788, Loss: 0.006180991884320974\n",
            "Epoch 789, Loss: 0.006180991884320974\n",
            "Epoch 790, Loss: 0.006180991884320974\n",
            "Epoch 791, Loss: 0.006180991884320974\n",
            "Epoch 792, Loss: 0.006180991884320974\n",
            "Epoch 793, Loss: 0.006180991884320974\n",
            "Epoch 794, Loss: 0.006180991884320974\n",
            "Epoch 795, Loss: 0.006180991884320974\n",
            "Epoch 796, Loss: 0.006180991884320974\n",
            "Epoch 797, Loss: 0.006180991884320974\n",
            "Epoch 798, Loss: 0.006180991884320974\n",
            "Epoch 799, Loss: 0.006180991884320974\n",
            "Epoch 800, Loss: 0.006180991884320974\n",
            "Epoch 801, Loss: 0.006180991884320974\n",
            "Epoch 802, Loss: 0.006180991884320974\n",
            "Epoch 803, Loss: 0.006180991884320974\n",
            "Epoch 804, Loss: 0.006180991884320974\n",
            "Epoch 805, Loss: 0.006180991884320974\n",
            "Epoch 806, Loss: 0.006180991884320974\n",
            "Epoch 807, Loss: 0.006180991884320974\n",
            "Epoch 808, Loss: 0.006180991884320974\n",
            "Epoch 809, Loss: 0.006180991884320974\n",
            "Epoch 810, Loss: 0.006180991884320974\n",
            "Epoch 811, Loss: 0.006180991884320974\n",
            "Epoch 812, Loss: 0.006180991884320974\n",
            "Epoch 813, Loss: 0.006180991884320974\n",
            "Epoch 814, Loss: 0.006180991884320974\n",
            "Epoch 815, Loss: 0.006180991884320974\n",
            "Epoch 816, Loss: 0.006180991884320974\n",
            "Epoch 817, Loss: 0.006180991884320974\n",
            "Epoch 818, Loss: 0.006180991884320974\n",
            "Epoch 819, Loss: 0.006180991884320974\n",
            "Epoch 820, Loss: 0.006180991884320974\n",
            "Epoch 821, Loss: 0.006180991884320974\n",
            "Epoch 822, Loss: 0.006180991884320974\n",
            "Epoch 823, Loss: 0.006180991884320974\n",
            "Epoch 824, Loss: 0.006180991884320974\n",
            "Epoch 825, Loss: 0.006180991884320974\n",
            "Epoch 826, Loss: 0.006180991884320974\n",
            "Epoch 827, Loss: 0.006180991884320974\n",
            "Epoch 828, Loss: 0.006180991884320974\n",
            "Epoch 829, Loss: 0.006180991884320974\n",
            "Epoch 830, Loss: 0.006180991884320974\n",
            "Epoch 831, Loss: 0.006180991884320974\n",
            "Epoch 832, Loss: 0.006180991884320974\n",
            "Epoch 833, Loss: 0.006180991884320974\n",
            "Epoch 834, Loss: 0.006180991884320974\n",
            "Epoch 835, Loss: 0.006180991884320974\n",
            "Epoch 836, Loss: 0.006180991884320974\n",
            "Epoch 837, Loss: 0.006180991884320974\n",
            "Epoch 838, Loss: 0.006180991884320974\n",
            "Epoch 839, Loss: 0.006180991884320974\n",
            "Epoch 840, Loss: 0.006180991884320974\n",
            "Epoch 841, Loss: 0.006180991884320974\n",
            "Epoch 842, Loss: 0.006180991884320974\n",
            "Epoch 843, Loss: 0.006180991884320974\n",
            "Epoch 844, Loss: 0.006180991884320974\n",
            "Epoch 845, Loss: 0.006180991884320974\n",
            "Epoch 846, Loss: 0.006180991884320974\n",
            "Epoch 847, Loss: 0.006180991884320974\n",
            "Epoch 848, Loss: 0.006180991884320974\n",
            "Epoch 849, Loss: 0.006180991884320974\n",
            "Epoch 850, Loss: 0.006180991884320974\n",
            "Epoch 851, Loss: 0.006180991884320974\n",
            "Epoch 852, Loss: 0.006180991884320974\n",
            "Epoch 853, Loss: 0.006180991884320974\n",
            "Epoch 854, Loss: 0.006180991884320974\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 91%|█████████ | 910/1000 [00:02<00:00, 497.60it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 855, Loss: 0.006180991884320974\n",
            "Epoch 856, Loss: 0.006180991884320974\n",
            "Epoch 857, Loss: 0.006180991884320974\n",
            "Epoch 858, Loss: 0.006180991884320974\n",
            "Epoch 859, Loss: 0.006180991884320974\n",
            "Epoch 860, Loss: 0.006180991884320974\n",
            "Epoch 861, Loss: 0.006180991884320974\n",
            "Epoch 862, Loss: 0.006180991884320974\n",
            "Epoch 863, Loss: 0.006180991884320974\n",
            "Epoch 864, Loss: 0.006180991884320974\n",
            "Epoch 865, Loss: 0.006180991884320974\n",
            "Epoch 866, Loss: 0.006180991884320974\n",
            "Epoch 867, Loss: 0.006180991884320974\n",
            "Epoch 868, Loss: 0.006180991884320974\n",
            "Epoch 869, Loss: 0.006180991884320974\n",
            "Epoch 870, Loss: 0.006180991884320974\n",
            "Epoch 871, Loss: 0.006180991884320974\n",
            "Epoch 872, Loss: 0.006180991884320974\n",
            "Epoch 873, Loss: 0.006180991884320974\n",
            "Epoch 874, Loss: 0.006180991884320974\n",
            "Epoch 875, Loss: 0.006180991884320974\n",
            "Epoch 876, Loss: 0.006180991884320974\n",
            "Epoch 877, Loss: 0.006180991884320974\n",
            "Epoch 878, Loss: 0.006180991884320974\n",
            "Epoch 879, Loss: 0.006180991884320974\n",
            "Epoch 880, Loss: 0.006180991884320974\n",
            "Epoch 881, Loss: 0.006180991884320974\n",
            "Epoch 882, Loss: 0.006180991884320974\n",
            "Epoch 883, Loss: 0.006180991884320974\n",
            "Epoch 884, Loss: 0.006180991884320974\n",
            "Epoch 885, Loss: 0.006180991884320974\n",
            "Epoch 886, Loss: 0.006180991884320974\n",
            "Epoch 887, Loss: 0.006180991884320974\n",
            "Epoch 888, Loss: 0.006180991884320974\n",
            "Epoch 889, Loss: 0.006180991884320974\n",
            "Epoch 890, Loss: 0.006180991884320974\n",
            "Epoch 891, Loss: 0.006180991884320974\n",
            "Epoch 892, Loss: 0.006180991884320974\n",
            "Epoch 893, Loss: 0.006180991884320974\n",
            "Epoch 894, Loss: 0.006180991884320974\n",
            "Epoch 895, Loss: 0.006180991884320974\n",
            "Epoch 896, Loss: 0.006180991884320974\n",
            "Epoch 897, Loss: 0.006180991884320974\n",
            "Epoch 898, Loss: 0.006180991884320974\n",
            "Epoch 899, Loss: 0.006180991884320974\n",
            "Epoch 900, Loss: 0.006180991884320974\n",
            "Epoch 901, Loss: 0.006180991884320974\n",
            "Epoch 902, Loss: 0.006180991884320974\n",
            "Epoch 903, Loss: 0.006180991884320974\n",
            "Epoch 904, Loss: 0.006180991884320974\n",
            "Epoch 905, Loss: 0.006180991884320974\n",
            "Epoch 906, Loss: 0.006180991884320974\n",
            "Epoch 907, Loss: 0.006180991884320974\n",
            "Epoch 908, Loss: 0.006180991884320974\n",
            "Epoch 909, Loss: 0.006180991884320974\n",
            "Epoch 910, Loss: 0.006180991884320974\n",
            "Epoch 911, Loss: 0.006180991884320974\n",
            "Epoch 912, Loss: 0.006180991884320974\n",
            "Epoch 913, Loss: 0.006180991884320974\n",
            "Epoch 914, Loss: 0.006180991884320974\n",
            "Epoch 915, Loss: 0.006180991884320974\n",
            "Epoch 916, Loss: 0.006180991884320974\n",
            "Epoch 917, Loss: 0.006180991884320974\n",
            "Epoch 918, Loss: 0.006180991884320974\n",
            "Epoch 919, Loss: 0.006180991884320974\n",
            "Epoch 920, Loss: 0.006180991884320974\n",
            "Epoch 921, Loss: 0.006180991884320974\n",
            "Epoch 922, Loss: 0.006180991884320974\n",
            "Epoch 923, Loss: 0.006180991884320974\n",
            "Epoch 924, Loss: 0.006180991884320974\n",
            "Epoch 925, Loss: 0.006180991884320974\n",
            "Epoch 926, Loss: 0.006180991884320974\n",
            "Epoch 927, Loss: 0.006180991884320974\n",
            "Epoch 928, Loss: 0.006180991884320974\n",
            "Epoch 929, Loss: 0.006180991884320974\n",
            "Epoch 930, Loss: 0.006180991884320974\n",
            "Epoch 931, Loss: 0.006180991884320974\n",
            "Epoch 932, Loss: 0.006180991884320974\n",
            "Epoch 933, Loss: 0.006180991884320974\n",
            "Epoch 934, Loss: 0.006180991884320974\n",
            "Epoch 935, Loss: 0.006180991884320974\n",
            "Epoch 936, Loss: 0.006180991884320974\n",
            "Epoch 937, Loss: 0.006180991884320974\n",
            "Epoch 938, Loss: 0.006180991884320974\n",
            "Epoch 939, Loss: 0.006180991884320974\n",
            "Epoch 940, Loss: 0.006180991884320974\n",
            "Epoch 941, Loss: 0.006180991884320974\n",
            "Epoch 942, Loss: 0.006180991884320974\n",
            "Epoch 943, Loss: 0.006180991884320974\n",
            "Epoch 944, Loss: 0.006180991884320974\n",
            "Epoch 945, Loss: 0.006180991884320974\n",
            "Epoch 946, Loss: 0.006180991884320974\n",
            "Epoch 947, Loss: 0.006180991884320974\n",
            "Epoch 948, Loss: 0.006180991884320974\n",
            "Epoch 949, Loss: 0.006180991884320974\n",
            "Epoch 950, Loss: 0.006180991884320974\n",
            "Epoch 951, Loss: 0.006180991884320974\n",
            "Epoch 952, Loss: 0.006180991884320974\n",
            "Epoch 953, Loss: 0.006180991884320974\n",
            "Epoch 954, Loss: 0.006180991884320974\n",
            "Epoch 955, Loss: 0.006180991884320974\n",
            "Epoch 956, Loss: 0.006180991884320974\n",
            "Epoch 957, Loss: 0.006180991884320974\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:02<00:00, 448.37it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 958, Loss: 0.006180991884320974\n",
            "Epoch 959, Loss: 0.006180991884320974\n",
            "Epoch 960, Loss: 0.006180991884320974\n",
            "Epoch 961, Loss: 0.006180991884320974\n",
            "Epoch 962, Loss: 0.006180991884320974\n",
            "Epoch 963, Loss: 0.006180991884320974\n",
            "Epoch 964, Loss: 0.006180991884320974\n",
            "Epoch 965, Loss: 0.006180991884320974\n",
            "Epoch 966, Loss: 0.006180991884320974\n",
            "Epoch 967, Loss: 0.006180991884320974\n",
            "Epoch 968, Loss: 0.006180991884320974\n",
            "Epoch 969, Loss: 0.006180991884320974\n",
            "Epoch 970, Loss: 0.006180991884320974\n",
            "Epoch 971, Loss: 0.006180991884320974\n",
            "Epoch 972, Loss: 0.006180991884320974\n",
            "Epoch 973, Loss: 0.006180991884320974\n",
            "Epoch 974, Loss: 0.006180991884320974\n",
            "Epoch 975, Loss: 0.006180991884320974\n",
            "Epoch 976, Loss: 0.006180991884320974\n",
            "Epoch 977, Loss: 0.006180991884320974\n",
            "Epoch 978, Loss: 0.006180991884320974\n",
            "Epoch 979, Loss: 0.006180991884320974\n",
            "Epoch 980, Loss: 0.006180991884320974\n",
            "Epoch 981, Loss: 0.006180991884320974\n",
            "Epoch 982, Loss: 0.006180991884320974\n",
            "Epoch 983, Loss: 0.006180991884320974\n",
            "Epoch 984, Loss: 0.006180991884320974\n",
            "Epoch 985, Loss: 0.006180991884320974\n",
            "Epoch 986, Loss: 0.006180991884320974\n",
            "Epoch 987, Loss: 0.006180991884320974\n",
            "Epoch 988, Loss: 0.006180991884320974\n",
            "Epoch 989, Loss: 0.006180991884320974\n",
            "Epoch 990, Loss: 0.006180991884320974\n",
            "Epoch 991, Loss: 0.006180991884320974\n",
            "Epoch 992, Loss: 0.006180991884320974\n",
            "Epoch 993, Loss: 0.006180991884320974\n",
            "Epoch 994, Loss: 0.006180991884320974\n",
            "Epoch 995, Loss: 0.006180991884320974\n",
            "Epoch 996, Loss: 0.006180991884320974\n",
            "Epoch 997, Loss: 0.006180991884320974\n",
            "Epoch 998, Loss: 0.006180991884320974\n",
            "Epoch 999, Loss: 0.006180991884320974\n",
            "Epoch 1000, Loss: 0.006180991884320974\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Обучение модели\n",
        "for epoch in tqdm(range(1000)):  # Пример цикла обучения\n",
        "    optimizer.zero_grad()  # Очистка градиентов\n",
        "    outputs = model(X_train_tensor)  # Прогноз модели\n",
        "    loss = criterion(outputs, y_train_tensor)  # Вычисление потерь\n",
        "    loss.backward()  # Обратное распространение ошибки\n",
        "    optimizer.step()  # Обновление весов\n",
        "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGkctA_-zzcq"
      },
      "source": [
        "Протестируем модель:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDxwFJmAzzcq",
        "outputId": "73865693-8bf0-4245-8928-a57e8528cedd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R^2 score: -0.009416054007495234\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "\n",
        "# Выполнение предсказаний на тестовых данных\n",
        "with torch.no_grad():\n",
        "    model.eval()  # Установка модели в режим оценки\n",
        "    y_pred = model(X_test_tensor)  # Предсказания модели\n",
        "    y_pred = y_pred.cpu().numpy()  # Конвертация предсказаний в NumPy массив\n",
        "\n",
        "# Вычисление R^2\n",
        "r2 = r2_score(y_test.to_numpy(), y_pred.reshape(1, -1)[0])\n",
        "print(f'R^2 score: {r2}')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
